{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "import plotly.express as px\n",
    "from pickle import dump\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df: pd.DataFrame)-> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Takes a CSV from querying DynamoDB and preprocesses the data to the \n",
    "    required format for input to training. \n",
    "\n",
    "    Args: \n",
    "        df: a dataframe of readings, including temperature and humidity.\n",
    "    \n",
    "    Returns: \n",
    "        df: a dataframe of temperature readings, sorted by timestamp\n",
    "    \"\"\"\n",
    "    df.rename(columns={'humidity.S': 'humidity',\n",
    "                   'temperature.S':'temperature',\n",
    "                   'timestamp.S':'timestamp'},inplace=True)\n",
    "    \n",
    "    # Convert the timestamp column to datetime format\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], format='mixed')\n",
    "\n",
    "    # Round the timestamp to the nearest minute\n",
    "    df['timestamp'] = df['timestamp'].dt.round('1min')\n",
    "\n",
    "    # Remove unnecessary columns\n",
    "    df.drop(columns=['Unnamed: 0', 'humidity'],inplace=True)\n",
    "    df.sort_values(by='timestamp')\n",
    "    df = df[ df['timestamp'] > '2023-04-28' ]\n",
    "    df.set_index('timestamp', inplace=True)\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    return pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_missing_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Augments the data by looking for gaps of over 15 minutes, then inserting past\n",
    "    temperatures from 24 hours ago. \n",
    "\n",
    "    Args:\n",
    "        df: a dataframe of temperature readings, sorted by timestamp\n",
    "\n",
    "    Returns:\n",
    "        df: a dataframe of temperature readings, sorted by timestamp, augmented\n",
    "        to input missing data with the value from 24 hours previous. \n",
    "    \"\"\"\n",
    "\n",
    "    print(f'Dataframe size before augmentation: {df.shape}')\n",
    "\n",
    "    df_original_shape = df.shape\n",
    "    one_day = 10 * 24\n",
    "    time_interval = timedelta(minutes=10)\n",
    "    i = one_day \n",
    "\n",
    "    while i < df.shape[0]-1: \n",
    "\n",
    "        current_time = pd.Timestamp(df.index[i])\n",
    "        next_time = pd.Timestamp(df.index[i + 1])\n",
    "        \n",
    "        if (next_time - current_time) > time_interval + timedelta(minutes=15):\n",
    "            \n",
    "            previous_value_temp = df.iloc[i+1-one_day]['temperature']    \n",
    "            new_row = pd.DataFrame({'temperature': previous_value_temp}, index=[pd.Timestamp(current_time + time_interval)])      \n",
    "            df = pd.concat([df.iloc[:i+1], new_row, df.iloc[i+1:]])\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    print(f'Dataframe size after empty rows added: {df.shape}')\n",
    "    print(f'Rows added, {df.shape[0]-df_original_shape[0]},' \n",
    "          f'or {np.round((df.shape[0]-df_original_shape[0])*100/df.shape[0],2)}% of the new total.')\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split(df: pd.DataFrame)-> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Takes a preprocessed dataframe, assumed to be sorted chronologically,\n",
    "    and returns three dataframes split into train, validation, and test; \n",
    "    done chronologically in a 60%/20%/20% split. \n",
    "\n",
    "    Args: \n",
    "        df: a DataFrame\n",
    "\n",
    "    Returns:\n",
    "        df_train, df_test, df_val: a tuple of the train, test, and validation \n",
    "        data.\n",
    "    \"\"\"\n",
    "\n",
    "    train_index = int(np.round(df.shape[0]*0.6))\n",
    "    val_index = int(np.round(df.shape[0]*0.8))\n",
    "\n",
    "    df_train = df.iloc[:train_index,]\n",
    "    df_val = df.iloc[train_index:val_index]\n",
    "    df_test = df.iloc[val_index:]\n",
    "\n",
    "    return df_train, df_test, df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(df_train, df_val, df_test):\n",
    "    \"\"\"Scales data and saves scaling object as a pickle file. \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    print(scaler.fit(df_train))\n",
    "    print(scaler.mean_)\n",
    "    print(scaler.scale_)\n",
    "\n",
    "    dump(scaler, open('scaler.pkl', 'wb'))\n",
    "\n",
    "    return scaler.transform(df_train), scaler.transform(df_val), scaler.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequences(df_train, df_val, df_test):\n",
    "    \"\"\"\n",
    "    Creating the generators. The task will be to take in one hour of readings, \n",
    "    spaced 10 minutes apart, and predict the temperature in two hours. \n",
    "    \n",
    "    For example, there will be readings at 3:00pm, 3:10pm, ..., 4:00pm, \n",
    "    and the task will be to predict the temperature at 6pm. \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    delay = 24\n",
    "    sequence_length = 12\n",
    "\n",
    "    train = keras.preprocessing.timeseries_dataset_from_array(df_train[:-delay], \n",
    "                df_train[sequence_length+delay:], sequence_length=sequence_length, \n",
    "                batch_size=1, shuffle=True)\n",
    "\n",
    "    validation = keras.preprocessing.timeseries_dataset_from_array(df_val[:-delay], \n",
    "                df_val[sequence_length+delay:], sequence_length=sequence_length, \n",
    "                batch_size=1, shuffle=True)\n",
    "\n",
    "    test = keras.preprocessing.timeseries_dataset_from_array(df_test[:-delay], \n",
    "                df_test[sequence_length+delay:], sequence_length=sequence_length, \n",
    "                batch_size=1, shuffle=True)\n",
    "    \n",
    "    return train, validation, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe size before augmentation: (16134, 1)\n",
      "Dataframe size after empty rows added: (17325, 1)\n",
      "Rows added, 1191,or 6.87% of the new total.\n",
      "StandardScaler()\n",
      "[22.68970659]\n",
      "[2.39348571]\n"
     ]
    }
   ],
   "source": [
    "def run_preprocessing_pipeline(csv_input_path: str)-> list:\n",
    "    \"\"\"\n",
    "    Run pipeline from CSV to generators ready for input to training. \n",
    "\n",
    "    Args:\n",
    "        csv_input_path: a string of the path to the location of the CSV\n",
    "\n",
    "    Returns: \n",
    "        A tuple (df_train, df_val, df_test), containing three generators \n",
    "        which load 12 inputs and 1 target, from a scaled dataset of temperature \n",
    "        readings. \n",
    "    \"\"\"\n",
    "\n",
    "    df_raw = pd.read_csv(csv_input_path)\n",
    "\n",
    "    df_preprocessed = preprocess_data(df_raw)\n",
    "    df_augmented = augment_missing_data(df_preprocessed)\n",
    "    df_train, df_val, df_test = train_val_test_split(df_augmented)\n",
    "    df_train, df_val, df_test = scale_data(df_train, df_val, df_test)\n",
    "\n",
    "    return generate_sequences(df_train, df_val, df_test)\n",
    "\n",
    "train, validation, test = run_preprocessing_pipeline('analysis/ddb_output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run when loading the scaler object back\n",
    "\n",
    "# from pickle import load\n",
    "# scaler_loaded = load(open('scaler.pkl', 'rb'))\n",
    "# s_test = scaler_loaded.transform(df_test)\n",
    "# np.array_equal(s_test, df_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
