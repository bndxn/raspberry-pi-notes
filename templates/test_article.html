{% extends 'base.html' %}

{% block content %}

<head>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body>

    <h2>Ordinary least squares - viewed three ways</h2>
    <p>Consider a task where you have some input data \(X\), and your task is to predict the output \(y\). This idea of a linear relationship between variables is found in many fields. 

        \[y = wx + b\]
        
        A more general form of this is where you can take in multiple inputs, \(x_1, x_2, ...\). We can use the same approach to find the relationship between multiple input variables and an output variable. 
        
        In linear regression, we work out the weights \(w_1, w_2, ...\), in the equation \(y = x_1w_1 + x_2w_2 + ... + b\).
        
        You may remember from school doing something like: 
        
        \[S_{xy} = \sum{xy} - \frac{\sum{x}\sum{y}}{n}\]
        
        \[S_{xx} = \sum{x^2} - \frac{(\sum{x})^2}{n}\]
        
        Then to find the coefficients in our \(y = wx + b\), we get out our calculators and do something like this:
        \(w = \frac{S_{xx}}{S_{xy}}\)
        \(b = \bar{y} - w\bar{x}\)
        
        
        <h3>Part 1: The geometric interpretation - projections</h3>
        
        Consider projecting a point \(\mathbf{b}\) onto the line given by the vector \(\mathbf{a}\). 
        
        If we do this, there’ll be an error,  \(\mathbf{e}=\mathbf{b}-\mathbf{p}\), shown by the dashed line, of the difference between the projected point and the original point 
        And the projected point \(\mathbf{p}\) will be somewhere along \(\mathbf{a}\), so it’ll be \(\mathbf{p}=\mathbf{\hat{x}}\mathbf{a}\)
        The closest point on the line \(\mathbf{a}\) to the original \(\mathbf{b}\) will be when there’s a 90 degree angle between \(\mathbf{p}\) and \(\mathbf{a}\). This is where the minimisation happens.

        Writing this idea down
        \[\mathbf{a}\perp(\mathbf{e})\]
        \[\mathbf{a}\perp(\mathbf{b}-\mathbf{p})\]
        \[\mathbf{a}\cdot(\mathbf{b}-\mathbf{p})=0\]

        We defined \(\mathbf{p}=\mathbf{\hat{x}}\mathbf{a}\), so 

        \[\mathbf{a}\cdot(\mathbf{b}-\mathbf{\hat{x}}\mathbf{a})=0\]
        \[\mathbf{a}^T\mathbf{b}-\mathbf{a}^T\mathbf{a}\mathbf{\hat{x}}=0\]

        \[\mathbf{\hat{x}} = \frac{\mathbf{a}^T\mathbf{b}}{\mathbf{a}^T\mathbf{a}}\]

        This would give us the factor \(\mathbf{\hat{x}}\) in \(\mathbf{p}=\mathbf{\hat{x}}\mathbf{a}\).
        An example 

        So if we want to project \((1,3)\) onto \((2,4)\), what does that look like?

        \[\mathbf{a}^T\mathbf{b} = [1, 3]\cdot[2,4] = [14]\]

        \[\mathbf{a}^T\mathbf{b} = [1, 3]\cdot[1,3]=[10]\]

        So then \(\mathbf{\hat{x}}=\frac{14}{10}=\frac{7}{5}\), and then \(\mathbf{p}=\frac{7}{5}[1,3]\)

        How do we project in general?

        If the equation to project specifically the vector \(\mathbf{b}\) was the following: 

        \[\mathbf{p} = \frac{\mathbf{a}^T\mathbf{b}}{\mathbf{a}^T\mathbf{a}}\mathbf{a}\]

        then the matrix to project any vector will be 

        \[\mathbf{P} = \frac{\mathbf{a}^T\mathbf{a}}{\mathbf{a}^T\mathbf{a}}\]

            <h3>Geometrical interpretation - projection as best fit</h3>

        Now consider we’re trying to find a line of best fit, for points (0, 6), (1,0), (2,0). 

        Our C + Dt  

        \[A = \begin{bmatrix} 1 & t_1 \\ 1 & t_2 \\ 1 & t_3 \end{bmatrix} = \begin{bmatrix} 1 & 0 \\ 1 & 1 \\ 1 & 2 \end{bmatrix}\]and \(\mathbf{b} = \begin{bmatrix} 6 \\ 0 \\ 0\end{bmatrix}\)

        Let's introduce the idea of the column space - the space created by taking different combinations/amounts of the columns in A. 

        We can interpret the column space as all the possible combinations of the two vectors.

        Now it would be great if there was a vector in the column space that we could multiply by the columns to get exactly \(\mathbf{b}\). But alas, we can’t. 
        Why does projection find best the point?
        So instead, we think of all possible combinations of columns. The one that would be the best for us is the one that is on the subspace but is still as close to \(\mathbf{b}\) as possible. 

        We want to find the point on the column space described by \(A\) which as close as possible to the vector \(\mathbf{b}\). We can find this out by projecting \(\mathbf{b}\) onto \(A\)!

        Previously we found the minimal distance by working out an error that was perpendicular to one line. 

        \[\mathbf{a}\cdot(\mathbf{b}-\mathbf{\hat{x}}\mathbf{a})=0\]

        This was for a vector \(\mathbf{a}\), and projecting \(\mathbf{b}\) onto a line. Now we want to project \(\mathbf{b}\) onto a space. 

        We want to write down a condition that the errors are minimised each time, and that will be for each occurrence. So for our 3x2 matrix above, there’ll be three conditions. We’re also no longer just projecting onto a line with \(\mathbf{\hat{x}a}\), instead it’s \(\mathbf{A\hat{x}}\).

        \[\mathbf{a_{1}}^{T}(\mathbf{b}-\mathbf{A}\mathbf{\hat{x}})=0\]
        \[\mathbf{a_{2}}^{T}(\mathbf{b}-\mathbf{A}\mathbf{\hat{x}})=0\]
        …
        \[\mathbf{a_{n}}^{T}(\mathbf{b}-\mathbf{A}\mathbf{\hat{x}})=0\]

        We could just put these all in one big matrix, this would be a matrix of \(A^T\). So we can now combine all of these conditions into one big \(A\):

        \[A^TA\mathbf{\hat{x}}=A^T\mathbf{b}\]

        <br>        
        <h4>Geometrical interpretation of projection</h4>
        There's a really nice geometrical interpretation here, which is that we're projecting a vector onto the column space. 
        
        And this column space could be very high dimensional, in this case I've shown 3 dimensions, but it could have many more. 
        
        So just to recap, this is 
        
        \[A^TA\mathbf{\hat{x}}=A^T\mathbf{b}\]
        
        This is our final result from projection.

        <h3>Part 2: The route from ordinary least squares</h3>
        
        In ordinary least squares, we minimise the square of the error. We’re going to formulate our straight line to be of the form \(b = C + Dt\), to avoid using any of \(A,B,X\) twice.
        
        What this means is that \(C\) is our constant term, and \(t\) is the input variable which changes. 
        
        \[A = \begin{bmatrix} 1 & t_1 \\ 1 & t_2 \\ 1 & t_3 \end{bmatrix} = \begin{bmatrix} 1 & 0 \\ 1 & 1 \\ 1 & 2 \end{bmatrix}\] 
        
        and 
        
        \[\mathbf{b} = \begin{bmatrix} 6 \\ 0 \\ 0\end{bmatrix}\]
        
        This corresponds to the data points (0, 6), (1,0), (2,0). 
        
        Let's write this as a sum over the \(n\) data points. 
        
        \[E(x)=\sum{(\mathbf{Ax-b})^2}\]
        
        \[E(x)=\sum{(C + Dt_i -b_i)^2}\]
        
        \[E(x)=\sum{[(C^2 + CDt_i - Cb_i) + (CDt_i + D^2t^2_i - Dt_ib_i) - (Cb_i + Dt_ib_i + b^2_i)]}\]
        
        We're going to minimise this error function by taking partial derivates, then combining the conditions, to give us an overall condition for the error to be minimised. 
        
        We start by taking partial derivatives with respect to \(C\)
        
        \[\frac{\partial E(x))}{\partial C} = \sum(2C + Dt_i - b_i + Dt_i - b_i) = 0\]
        
        $$\sum(2C + 2Dt_i) = \sum{2b_i}$$
        $$\sum{(C + Dt_i)} = \sum{b_i}$$ 
        $$nC + D\sum{t_i} = \sum{b_i}$$
        
        Now we look at the condition to minimise the error with respect to $$D$$
        
        $$\frac{\partial E(x))}{\partial D} = \sum(Ct_i + Ct_i + 2Dt^2_i - t_ib_i - t_ib_i) = 0$$
        
        Reducing this gives us 
        
        $$\sum{(2Ct_i + 2Dt^2_i - 2t_ib_i)} = 0$$
        $$\sum{(Ct_i + Dt^2_i)} = \sum{t_ib_i}$$  
        $$C\sum{t_i} + D\sum{t^2_i} = \sum{t_ib_i}$$ (1)
        
        So now we have two conditions: 
        
        $$nC + D\sum{t_i} = \sum{b_i}$$
        $$C\sum{t_i} + D\sum{t^2_i} = \sum{t_ib_i}$$
        
        So interestingly, both of these have factors of \(C, D\) on the left hand side, and we can take these out and write it in matrix form: 
        
        $$\begin{bmatrix} m & \sum{t_i} \\ \sum{t_i} & \sum{t^2_i}\end{bmatrix} \begin{bmatrix} C \\ D \end{bmatrix} = \begin{bmatrix} \sum{b_i} \\ \sum{t_ib_i} \end{bmatrix}$$
        
        So this expresses the joint conditions that have to be true. 
        
        Let’s look at this expression and start with the matrix on the left - it’s interesting that it’s symmetric, the same down the diagonals.
        
        $$\begin{bmatrix} m & \sum{t_i} \\ \sum{t_i} & \sum{t^2_i}\end{bmatrix}$$
        We can unpack this as the product of two matrices $$\begin{bmatrix} 1 & 1 & ... & 1 \\ t_1 & t_2 & ... & t_n \end{bmatrix} \begin{bmatrix} 1 & t_1 \\ 1 & t_2 \\ ... & ... \\ 1 & t_n \end{bmatrix}$$
        And the right hand side, we be expressed as two matrices as well. 
        
        $$\begin{bmatrix} 1 & 1 & ... & 1 \\ t_1 & t_2 & ... & t_n \end{bmatrix}\begin{bmatrix} b_1 \\ b_2 \\ ... \\ b_n \end{bmatrix}$$
        
        So the conditions from calculus were for this equality to hold: 
        
        $$\begin{bmatrix} m & \sum{t_i} \\ \sum{t_i} & \sum{t^2_i}\end{bmatrix} \begin{bmatrix} C \\ D \end{bmatrix} = \begin{bmatrix} \sum{b_i} \\ \sum{t_ib_i} \end{bmatrix}$$
        
        And because we can decompose it into \(A^TA\mathbf{\hat{x}}=A^T\mathbf{b}\), it means the results are the same. 
        
        <h3>Part 3: Link between calculus and dot product</h3>
        Our task was to take a matrix of data, and to find the line of best fit. We wanted to find the vector \(\mathbf{\hat{x}} = [x_1, x_2]^T\)  that we can multiply by \(A\), to get the best fit for \(\mathbf{b}\).
        
        $$A = \begin{bmatrix} 1 & 0 \\ 1 & 1 \\ 1 & 2 \end{bmatrix}$$ and $$\mathbf{b} = \begin{bmatrix} 6 \\ 0 \\ 0\end{bmatrix}$$
        
        So what we found from the geometric route was that if we wanted to get the closest possible, we can think about this as projecting the point \(\mathbf{b}\) onto the space described by the columns of \(A\).  
        
        $$A^TA\mathbf{\hat{x}}=A^T\mathbf{b}$$
        
        This was the same as the answer through calculus. When we worked out what the errors would be, summed them up, then differentiated with respect to each our free parameters (the weight and the bias term), we also got $$A^TA\mathbf{\hat{x}}=A^T\mathbf{b}$$.
        
        <h3>Part 4: Link to A-level maths example</h3>
        Often the data matrix is called \(X\), the weights or coefficients are \(\mathbf{w}\), and the output vector is \(\mathbf{y}\).
        
        $$X^TX\mathbf{\hat{w}}=X^T\mathbf{y}$$
        
        Can we relate this to the \(S_{xx}\) and \(S_{xy}\) back from A-level? We can rewrite this in terms of the weights:
        
        $$\mathbf{\hat{w}}=(X^TX)^{-1}X^T\mathbf{y}$$
        
        $$\mathbf{w} = \begin{bmatrix} w_1 \\ w_2 \end{bmatrix} = (\begin{bmatrix} 1 & 1 & ... & 1 \\ x_1 & x_2 & ... & x_n \end{bmatrix} \begin{bmatrix} 1 & x_1 \\ 1 & x_2 \\ ... & ... \\ 1 & x_n \end{bmatrix})^{-1}X^T\mathbf{y}$$ 
        
        $$\mathbf{w}= (\begin{bmatrix} n & \sum{x_i} \\ \sum{x_i} & \sum{x^2_i}\end{bmatrix})^{-1}X^{T}\mathbf{y}$$
        
        Using the formula for the inverse of a 2x2 matrix, 
        
        $$\mathbf{w} = 1/(n\sum{x_i}^2-(\sum{x_i})^2) \begin{bmatrix} \sum{x^2_i} & -\sum{x_i} \\ -\sum{x_i} & n \end{bmatrix}^{-1}X^{T}\mathbf{y}$$
        
        Let’s just focus on \(X^{T}\mathbf{y}\) for a moment
        
        $$X^T\mathbf{y} = \begin{bmatrix} 1 & 1 & ... & 1 \\ x_1 & x_2 & ... & x_n \end{bmatrix} \begin{bmatrix} y_1 \\ y_2 \\ ... \\ y_n\end{bmatrix} = \begin{bmatrix} \sum{y} \\ \sum{xy} \end{bmatrix}$$
        
        Ok we put that back in to the expression for \(\mathbf{w}\), 
        
        $$\mathbf{w} = 1/(...) \begin{bmatrix} \sum{x^2_i} & -\sum{x_i} \\ -\sum{x_i} & n \end{bmatrix}^{-1}\begin{bmatrix} \sum{y} \\ \sum{xy} \end{bmatrix}$$
        
        $$\mathbf{w} = 1/(...) \begin{bmatrix} \sum{x_i^2y_i} - \sum{x_i^2y_i} \\ -\sum{x_i}\sum{y_i} + n\sum{xy} \end{bmatrix}$$
        
        $$w_2 = (n\sum{xy} -\sum{x_iy_i}) / (n\sum{x_i}^2-(\sum{x_i})^2)$$
        
        $$w_2 = (\sum{x}\sum{y} -\frac{1}{n}\sum{x_iy_i}) / (\sum{x_i}^2-\frac{1}{n}(\sum{x_i})^2)$$
        
        And what was our A-level maths answer?
        
        $$S_{xy} = \sum{xy} - \frac{\sum{x}\sum{y}}{n}$$
        
        $$S_{xx} = \sum{x^2} - \frac{(\sum{x})^2}{n}$$
        
        These are the same!
        
        <h3>Conclusion</h3>
        So to conclude, 

        <ul>
        <li>From projecting a point onto a (hyper)plane, we get: \(A^TA\mathbf{\hat{x}}=A^T\mathbf{b}\)</li>
        <li>From finding the minimum point for ordinary least squares, we get \(A^TA\mathbf{\hat{x}}=A^T\mathbf{b}\)</li>
        <li>From A-level Maths, our answer is equivalent to \(X^TX\mathbf{\hat{w}}=X^T\mathbf{y}\), or written with different letters,
             \(A^TA\mathbf{\hat{x}}=A^T\mathbf{b}\)</li>
    </ul>
            </p>
</body>

{% endblock %}
